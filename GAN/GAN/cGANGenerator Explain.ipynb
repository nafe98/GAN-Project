{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b19ba10",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd527fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86de1a",
   "metadata": {},
   "source": [
    "# GPU Setting Parallel Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98890166",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.enable_eager_execution(config=config)\n",
    "layers = tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3207b",
   "metadata": {},
   "source": [
    "# Defining Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91276cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe architecture of generator is a modified U-Net.\\nThere are skip connections between the encoder and decoder (as in U-Net).\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The architecture of generator is a modified U-Net.\n",
    "There are skip connections between the encoder and decoder (as in U-Net).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca00a0",
   "metadata": {},
   "source": [
    "# EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2f6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.Model):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides_s = 2, apply_batchnorm=True, add = False, padding_s = 'same'):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=0.02)\n",
    "        conv = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides_s,\n",
    "                             padding=padding_s, kernel_initializer=initializer, use_bias=False)\n",
    "        ac = layers.LeakyReLU()\n",
    "        self.encoder_layer = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        if add:\n",
    "            self.encoder_layer = tf.keras.Sequential([conv])\n",
    "        elif apply_batchnorm:\n",
    "            bn = layers.BatchNormalization()\n",
    "            self.encoder_layer = tf.keras.Sequential([conv, bn, ac])\n",
    "        else:\n",
    "            self.encoder_layer = tf.keras.Sequential([conv, ac])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.encoder_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc37c2",
   "metadata": {},
   "source": [
    "# DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee579348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.Model):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides_s = 2, apply_dropout=False, add = False):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=0.02)\n",
    "        dconv = layers.Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides_s,\n",
    "                                       padding='same', kernel_initializer=initializer, use_bias=False)\n",
    "        bn = layers.BatchNormalization()\n",
    "        ac = layers.ReLU()\n",
    "        self.decoder_layer = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if add:\n",
    "            self.decoder_layer = tf.keras.Sequential([dconv])      \n",
    "        \n",
    "        \n",
    "        elif apply_dropout:\n",
    "            drop = layers.Dropout(rate=0.5)\n",
    "            self.decoder_layer = tf.keras.Sequential([dconv, bn, drop, ac])\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            self.decoder_layer = tf.keras.Sequential([dconv, bn, ac])\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    def call(self, x):\n",
    "        return self.decoder_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeacd34",
   "metadata": {},
   "source": [
    "# Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092cb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Resize Input\n",
    "        p_layer_1 = DecoderLayer(filters=2, kernel_size=4, strides_s = 2, apply_dropout=False, add = True) \n",
    "        p_layer_2  = DecoderLayer(filters=2, kernel_size=4, strides_s = 2, apply_dropout=False, add = True)\n",
    "        p_layer_3  = EncoderLayer(filters=2, kernel_size=(6,1),strides_s = (4,1), apply_batchnorm=False, add = True)\n",
    "        \n",
    "        self.p_layers = [p_layer_1,p_layer_2,p_layer_3]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        #encoder\n",
    "        encoder_layer_1 = EncoderLayer(filters=64*1,  kernel_size=4,apply_batchnorm=False)   \n",
    "        encoder_layer_2 = EncoderLayer(filters=64*2, kernel_size=4)       \n",
    "        encoder_layer_3 = EncoderLayer(filters=64*4, kernel_size=4)       \n",
    "        encoder_layer_4 = EncoderLayer(filters=64*8, kernel_size=4)       \n",
    "        encoder_layer_5 = EncoderLayer(filters=64*8, kernel_size=4)       \n",
    "        self.encoder_layers = [encoder_layer_1, encoder_layer_2, encoder_layer_3, encoder_layer_4,\n",
    "                               encoder_layer_5]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # deconder\n",
    "        decoder_layer_1 = DecoderLayer(filters=64*8, kernel_size=4, apply_dropout=True)   \n",
    "        decoder_layer_2 = DecoderLayer(filters=64*8, kernel_size=4,apply_dropout=True)   \n",
    "        decoder_layer_3 = DecoderLayer(filters=64*8, kernel_size=4, apply_dropout=True)   \n",
    "        decoder_layer_4 = DecoderLayer(filters=64*4, kernel_size=4)   \n",
    "        self.decoder_layers = [decoder_layer_1, decoder_layer_2, decoder_layer_3, decoder_layer_4]\n",
    "\n",
    "       \n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=0.02)\n",
    "        self.last = layers.Conv2DTranspose(filters=2, kernel_size=4, strides=2, padding='same',\n",
    "                                           kernel_initializer=initializer, activation='tanh') #tanh output image will be between -1 and 1\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # pass the encoder and record xs\n",
    "        for p_layer in self.p_layers:\n",
    "            x = p_layer(x)\n",
    "\n",
    "        encoder_xs = []\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "            encoder_xs.append(x)\n",
    "        encoder_xs = encoder_xs[:-1][::-1]    # reverse\n",
    "        assert len(encoder_xs) == 4\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # pass the decoder and apply skip connection\n",
    "        for i, decoder_layer in enumerate(self.decoder_layers):\n",
    "            x = decoder_layer(x)\n",
    "            x = tf.concat([x, encoder_xs[i]], axis=-1)     # skip connect\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.last(x)        # last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5f8be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
